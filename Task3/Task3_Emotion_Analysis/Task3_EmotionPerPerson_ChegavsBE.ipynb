{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template for emotion analysis\n",
    "\n",
    "- This serves the purpose of correctly identifying the two politicians throughout each frame of a given debate and correctly matching the corresponding identified emotion.\n",
    "\n",
    "- The bounding box coordinates approach will be used for frames where the two politicians are in frame.\n",
    "\n",
    "- When only one politician is in frame, facial embedding vectors will be used and clustered. However, if the result of the clustering approaches is not reasonable enough, those frames will be discarded for the analysis (which correspond to moreless 40% of the debate)\n",
    "\n",
    "\n",
    "## Instructions:\n",
    "1. Download debate frames folder (not processed) and include it in __.gitignore file__!\n",
    "2. Check if the video has sign language interpreter or not. If not readjust 4, 3 and 2 'people on frame' if conditions to 3, 2 and 1.\n",
    "3. Follow the numbered instructions throughout the code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mtick\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "###########################################\n",
    "##########  0. SELECT THE DEBATE ##########\n",
    "\n",
    "data = pd.read_pickle(\"../../pkl_files/chega-be.pkl\")\n",
    "###########################################\n",
    "\n",
    "\n",
    "# Order the frames by name\n",
    "data = data.sort_values(by='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store all embedding vectors\n",
    "embedding_list = []\n",
    "\n",
    "\n",
    "########################################\n",
    "##########  1. SELECT THE PEOPLE #######\n",
    "\n",
    "# Each list will store the fer of each person in the frame\n",
    "# and the corresponding frame number\n",
    "\n",
    "Politician_1 = [] # Left side of the screen\n",
    "Politician_2 = [] # Right side of the screen\n",
    "#TV_Host = []\n",
    "#Sign_Language_Interpreter = []\n",
    "########################################\n",
    "\n",
    "\n",
    "# Collect all facial embedding vectors from the 'fer' column of each frame\n",
    "for i in range(data.shape[0]):\n",
    "\n",
    "    # Debate starts at frame 60 (minute 1)\n",
    "    if i >= 60:\n",
    "\n",
    "        # 4 people on frame\n",
    "        if len(data.iloc[i]['fer']) == 4: \n",
    "\n",
    "            # Initialize a empty list to store each person in the frame\n",
    "            people = []\n",
    "\n",
    "            # Iterate through each face in the frame \n",
    "            for fer in data.iloc[i]['fer']:\n",
    "                people.append(fer)\n",
    "\n",
    "            # Sort the people by the x-coordinate of the bounding box\n",
    "            people = sorted(people, key=lambda x: x['location'][0])\n",
    "\n",
    "            # Append the fer of each person to the corresponding list\n",
    "            Politician_1.append((people[0], i))\n",
    "            # TV_Host.append(people[1], i) -- not needed\n",
    "            Politician_2.append((people[2], i))\n",
    "            # Sign_Language_Interpreter.append(people[3], i) -- not needed\n",
    "\n",
    "\n",
    "        # 3 people on frame\n",
    "        elif len(data.iloc[i]['fer']) == 3:\n",
    "\n",
    "            # Initialize a empty list to store each person in the frame\n",
    "            people = []\n",
    "\n",
    "            # Iterate through each face in the frame \n",
    "            for fer in data.iloc[i]['fer']:\n",
    "                people.append(fer)\n",
    "\n",
    "            # Sort the people by the x-coordinate of the bounding box\n",
    "            people = sorted(people, key=lambda x: x['location'][0])\n",
    "\n",
    "            # Append the fer of each person to the corresponding list\n",
    "            Politician_1.append((people[0], i))\n",
    "            Politician_2.append((people[1], i))\n",
    "            # Sign_Language_Interpreter.append(people[2], i) -- not needed\n",
    "\n",
    "\n",
    "        # 2 people on frame (one is the sign language interpreter)\n",
    "        elif len(data.iloc[i]['fer']) == 2:\n",
    "            # Initialize a empty list to store each person in the frame\n",
    "            people = []\n",
    "\n",
    "            # Iterate through each face in the frame \n",
    "            for fer in data.iloc[i]['fer']:\n",
    "                people.append(fer)\n",
    "\n",
    "            # Sort the people by the x-coordinate of the bounding box\n",
    "            people = sorted(people, key=lambda x: x['location'][0])\n",
    "\n",
    "            # Append the fer of each person to the corresponding list\n",
    "            embedding_list.append(people[0]['embedding'])\n",
    "            # Sign_Language_Interpreter.append(people[1], i) -- not needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "\n",
    "- Now we have successfully applied the bounding box method.\n",
    "\n",
    "- We've also stored the embeddings for 2 faces frames (or one face frames if there isn't sign language interpreter). Now we have to cluster them. \n",
    "\n",
    "- Obviously there will be frames where the person that appears is the TV host and not one of the politicians. But the ratio is really small. We can also use 3 cluster instead of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of embeddings into a NumPy array\n",
    "embedding_array = np.array(embedding_list)\n",
    "\n",
    "########################################\n",
    "##########  2. CLUSTERING METHODS ######\n",
    "num_clusters = 2\n",
    "\n",
    "# Apply K-means clustering or Spectral clustering (see the best method for your data)\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "cluster_labels = kmeans.fit_predict(embedding_array)\n",
    "\n",
    "#spectral = SpectralClustering(n_clusters=num_clusters)\n",
    "#cluster_labels = spectral.fit_predict(embedding_array)\n",
    "########################################\n",
    "\n",
    "\n",
    "# Reduce the dimensionality of the embedding vectors using PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization\n",
    "embedding_pca = pca.fit_transform(embedding_array)\n",
    "\n",
    "# Plot clusters in 2D space after PCA\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(num_clusters):\n",
    "    plt.scatter(embedding_pca[cluster_labels == i, 0], embedding_pca[cluster_labels == i, 1], label=f'Cluster {i}', alpha=1-num_clusters*0.1, s=10)\n",
    "plt.title('Clustering Results (PCA) - Number of Clusters: ' + str(num_clusters))\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to match cluster number to person name\n",
    "cluster_to_person = {\n",
    "    0: 'Person 0',\n",
    "    1: 'Person 1'\n",
    "}\n",
    "\n",
    "# Create a dictionary to store the faces for each person\n",
    "faces_by_person = {person: [] for person in cluster_to_person.values()}\n",
    "\n",
    "# Iterate through each frame in the dataset\n",
    "for i in range(data.shape[0]):\n",
    "    # Decide which frames to plot (one frame every 10 frames)\n",
    "    if i % 10 == 0 and i >= 60:\n",
    "\n",
    "        # Only consider 2 faces frames\n",
    "        if len(data.iloc[i]['fer']) == 2:\n",
    "\n",
    "            # Iterate through each face in the frame\n",
    "            for fer in data.iloc[i]['fer']:\n",
    "                # Look for embedding vector of the face in embedding_array\n",
    "                embedding = fer['embedding']\n",
    "                indices = np.where((embedding_array == embedding).all(axis=1))[0]\n",
    "\n",
    "                # Check if indices is not empty\n",
    "                if indices.size > 0:\n",
    "                    embedding_index = indices[0]\n",
    "                else:\n",
    "                    continue  # Skip to the next iteration if indices is empty\n",
    "\n",
    "                # Look for cluster number of the face in cluster_labels\n",
    "                cluster_number = cluster_labels[embedding_index]\n",
    "\n",
    "                # Look for person name of the cluster number in cluster_to_person\n",
    "                person_name = cluster_to_person[cluster_number]\n",
    "\n",
    "                # Add the filename to the face dictionary\n",
    "                fer['filename'] = data.iloc[i]['filename']\n",
    "\n",
    "                # Add the face to the corresponding person's list\n",
    "                faces_by_person[person_name].append(fer)\n",
    "\n",
    "# Create lists to store the cropped images of each face\n",
    "cropped_images = []\n",
    "\n",
    "# Iterate through each person\n",
    "for person_name, faces in faces_by_person.items():\n",
    "    print(f\"# {person_name}\")\n",
    "\n",
    "    # Iterate through each face\n",
    "    for fer in faces:\n",
    "        ########################################\n",
    "        ##########  3. PLOT THE FACES ##########\n",
    "\n",
    "        # Plot frame image from the right folder!\n",
    "        img = Image.open(os.path.join('../../debates/chega-be/', fer['filename']))\n",
    "        ########################################\n",
    "\n",
    "        # Extract the bounding box coordinates\n",
    "        x = fer['location'][0]\n",
    "        y = fer['location'][2]\n",
    "        width = fer['location'][1] - fer['location'][0]\n",
    "        height = fer['location'][3] - fer['location'][2]\n",
    "\n",
    "        # Crop the image to the bounding box\n",
    "        cropped_img = img.crop((x, y, x + width, y + height))\n",
    "\n",
    "        # Add the cropped image to the list\n",
    "        cropped_images.append(cropped_img)\n",
    "\n",
    "    # Determine the number of images and the number of images per row\n",
    "    num_images = len(cropped_images)\n",
    "    images_per_row = 10  # Adjust as needed\n",
    "\n",
    "    # Adjust the number of images to be a multiple of images_per_row\n",
    "    num_images = (num_images // images_per_row) * images_per_row\n",
    "\n",
    "    # Determine the number of rows\n",
    "    num_rows = num_images // images_per_row\n",
    "\n",
    "    # Create a figure with a subplot for each image\n",
    "    fig, axs = plt.subplots(num_rows, images_per_row, figsize=(images_per_row, num_rows))  # Adjust size as needed\n",
    "\n",
    "    # Iterate through each image\n",
    "    for i, cropped_img in enumerate(cropped_images[:num_images]):\n",
    "        # Determine the row and column indices of the subplot\n",
    "        row = i // images_per_row\n",
    "        col = i % images_per_row\n",
    "\n",
    "        # Display the cropped image in the corresponding subplot\n",
    "        axs[row, col].imshow(cropped_img)\n",
    "        axs[row, col].axis('off')  # Hide axes\n",
    "\n",
    "    # Display all images\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Clear the list of cropped images for the next person\n",
    "    cropped_images.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision time\n",
    "Now we have to decide if the clustering was good enough and use these frames for emotion vs time analysis or if we simply discard them.\n",
    "So run or not the following code section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "##########  4. INCLUDE CLUSTERING? #####\n",
    "\n",
    "# Run this code only if you want to use 2 face frames (clustering is good enough)\n",
    "########################################\n",
    "\n",
    "# Create a dictionary to map embeddings to their cluster labels\n",
    "embedding_to_label = {tuple(embedding): label for embedding, label in zip(embedding_array, cluster_labels)}\n",
    "\n",
    "# Create a dictionary to map cluster labels to person names\n",
    "label_to_person = {label: person for person, label in cluster_to_person.items()}\n",
    "\n",
    "# Iterate through each frame in the dataset\n",
    "for i in range(data.shape[0]):\n",
    "    for fer in data.iloc[i]['fer']:\n",
    "        # Look for the cluster label of the face's embedding\n",
    "        embedding = tuple(fer['embedding'])\n",
    "        if embedding in embedding_to_label:\n",
    "            cluster_label = embedding_to_label[embedding]\n",
    "\n",
    "            # Look for the person name of the cluster label\n",
    "            if cluster_label in label_to_person:\n",
    "                person_name = label_to_person[cluster_label]\n",
    "\n",
    "                ################################################\n",
    "                ###### 5. ADD THE FACE TO THE RIGHT LIST #######\n",
    "                # Add the face to the appropriate list\n",
    "                if person_name == 'Person 0':\n",
    "                    Politician_1.append((fer, i))\n",
    "                elif person_name == 'Person 1':\n",
    "                    Politician_2.append((fer, i))\n",
    "\n",
    "                ################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Status:\n",
    "Now we have every politician list filled with its corresponding fer's. Let's order them by frame number in order to correctly evaluate their emotions through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order Politician_1 and Politician_2 by frame number\n",
    "Politician_1 = sorted(Politician_1, key=lambda x: x[1])\n",
    "Politician_2 = sorted(Politician_2, key=lambda x: x[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final step: emotion analysis\n",
    "\n",
    "Now we have all the fer's divided by politician and in chronological order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the faces of Politician_1 and Politician_2 (just to check if people identified are correct)\n",
    "\n",
    "# Create list of images\n",
    "cropped_images = []\n",
    "\n",
    "# Iterarte over dataset and check the fer's of each frame\n",
    "# If the fer is in Politician_1 or Politician_2, open the image and plot the face\n",
    "for i in range(data.shape[0]):\n",
    "    if i % 50 == 0 and i >= 60:\n",
    "        img = Image.open(os.path.join('../../debates/chega-be/', data.iloc[i]['filename']))\n",
    "        for fer in data.iloc[i]['fer']:\n",
    "            if fer in [x[0] for x in Politician_1]:\n",
    "                x = fer['location'][0]\n",
    "                y = fer['location'][2]\n",
    "                width = fer['location'][1] - fer['location'][0]\n",
    "                height = fer['location'][3] - fer['location'][2]\n",
    "                cropped_img = img.crop((x, y, x + width, y + height))\n",
    "                # Add the cropped image to the list\n",
    "                cropped_images.append(cropped_img)\n",
    "\n",
    "# Determine the number of images and the number of images per row\n",
    "num_images = len(cropped_images)\n",
    "images_per_row = 10  # Adjust as needed\n",
    "\n",
    "# Adjust the number of images to be a multiple of images_per_row\n",
    "num_images = (num_images // images_per_row) * images_per_row\n",
    "\n",
    "# Determine the number of rows\n",
    "num_rows = num_images // images_per_row\n",
    "\n",
    "# Create a figure with a subplot for each image\n",
    "fig, axs = plt.subplots(num_rows, images_per_row, figsize=(images_per_row, num_rows))  # Adjust size as needed\n",
    "\n",
    "# Iterate through each image\n",
    "for i, cropped_img in enumerate(cropped_images[:num_images]):\n",
    "    # Determine the row and column indices of the subplot\n",
    "    row = i // images_per_row\n",
    "    col = i % images_per_row\n",
    "\n",
    "    # Display the cropped image in the corresponding subplot\n",
    "    axs[row, col].imshow(cropped_img)\n",
    "    axs[row, col].axis('off')  # Hide axes\n",
    "\n",
    "# Display all images\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clear the list of cropped images for the next person\n",
    "cropped_images.clear()\n",
    "            \n",
    "for i in range(data.shape[0]):\n",
    "    if i % 50 == 0 and i >= 60:\n",
    "        img = Image.open(os.path.join('../../debates/chega-be/', data.iloc[i]['filename']))\n",
    "        for fer in data.iloc[i]['fer']:\n",
    "            if fer in [x[0] for x in Politician_2]:\n",
    "                x = fer['location'][0]\n",
    "                y = fer['location'][2]\n",
    "                width = fer['location'][1] - fer['location'][0]\n",
    "                height = fer['location'][3] - fer['location'][2]\n",
    "                cropped_img = img.crop((x, y, x + width, y + height))\n",
    "                # Add the cropped image to the list\n",
    "                cropped_images.append(cropped_img)\n",
    "\n",
    "# Determine the number of images and the number of images per row\n",
    "num_images = len(cropped_images)\n",
    "\n",
    "# Adjust the number of images to be a multiple of images_per_row\n",
    "num_images = (num_images // images_per_row) * images_per_row\n",
    "\n",
    "# Determine the number of rows\n",
    "num_rows = num_images // images_per_row\n",
    "\n",
    "# Create a figure with a subplot for each image\n",
    "fig, axs = plt.subplots(num_rows, images_per_row, figsize=(images_per_row, num_rows))  # Adjust size as needed\n",
    "\n",
    "# Iterate through each image\n",
    "for i, cropped_img in enumerate(cropped_images[:num_images]):\n",
    "    # Determine the row and column indices of the subplot\n",
    "    row = i // images_per_row\n",
    "    col = i % images_per_row\n",
    "\n",
    "    # Display the cropped image in the corresponding subplot\n",
    "    axs[row, col].imshow(cropped_img)\n",
    "    axs[row, col].axis('off')  # Hide axes\n",
    "\n",
    "# Display all images\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to count emotions and give a score\n",
    "\n",
    "1. Consider the count of each emotion for a politician. We can calculate their emotional score using the following formula:\n",
    "\n",
    "Emotional Score=(4xNeutral Count)+(5×Happiness Count)+(3×Surprise Count)−(2×Anger Count)−(2×Fear Count)−(2×Contempt Count)−(1×Sadness Count)−(2×Disgust Count)\n",
    "\n",
    "\n",
    "2. Then we divide this value by total number of identified emotions of the given politician in the debate: average emotion score for the debate.\n",
    "\n",
    "3. This value is the main goal of this script. We will use it in another one to sum and compute the total score of the politician.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_emotions(array_emotions):\n",
    "    # Extract only the dictionaries\n",
    "    politician_data_ilbe_MM = [data_point[0] for data_point in array_emotions]\n",
    "\n",
    "\n",
    "    # List of all possible emotions\n",
    "    emotions = ['Anger', 'Surprise', 'Fear', 'Contempt', 'Neutral', 'Sadness', 'Disgust', 'Happiness']\n",
    "\n",
    "    # Initialize a dictionary to hold the counts of each emotion\n",
    "    emotion_counts = {emotion: 0 for emotion in emotions}\n",
    "\n",
    "    # Loop through the data and count each emotion\n",
    "    for data_point in politician_data_ilbe_MM:\n",
    "        emotion = data_point['emotion']\n",
    "        if emotion in emotion_counts:\n",
    "            emotion_counts[emotion] += 1\n",
    "\n",
    "    return emotion_counts\n",
    "\n",
    "def calculate_emotional_score(emotion_counts, length):\n",
    "    # Define the weights for each emotion\n",
    "    weights = {\n",
    "        'Happiness': 5,\n",
    "        'Surprise': 3,\n",
    "        'Anger': -2,\n",
    "        'Fear': -2,\n",
    "        'Contempt': -2,\n",
    "        'Sadness': -1,\n",
    "        'Disgust': -2,\n",
    "        'Neutral': 4\n",
    "    }\n",
    "    \n",
    "    # Calculate the emotional score\n",
    "    score = sum(weights[emotion] * count for emotion, count in emotion_counts.items())\n",
    "    \n",
    "    return score/length\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final score\n",
    "\n",
    "- Now we print the count of emotions of each politician and give the right score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the emotions of each person and their emotional score\n",
    "print(\"Mortagua emotions: \", count_emotions(Politician_1))\n",
    "print(\"Ventura emotions: \", count_emotions(Politician_2))\n",
    "\n",
    "# Compute the emotional score for each person\n",
    "print(\"\\nMortagua emotional score: \", calculate_emotional_score(count_emotions(Politician_1), len(Politician_1)))\n",
    "print(\"Ventura emotional score: \", calculate_emotional_score(count_emotions(Politician_2), len(Politician_2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
